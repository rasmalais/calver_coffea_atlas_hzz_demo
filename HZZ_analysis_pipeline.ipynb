{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23ee171",
   "metadata": {},
   "source": [
    "# ATLAS Open Data $H\\rightarrow ZZ^\\star$ with `ServiceX`, `coffea`, `cabinetry` & `pyhf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c18542a-d978-4e17-8b66-04b5c38f4d09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: func_adl_servicex in /usr/local/lib/python3.10/site-packages (2.2)\n",
      "Requirement already satisfied: func-adl>=3.2 in /usr/local/lib/python3.10/site-packages (from func_adl_servicex) (3.3.1)\n",
      "Requirement already satisfied: qastle>=0.10 in /usr/local/lib/python3.10/site-packages (from func_adl_servicex) (0.17.0)\n",
      "Requirement already satisfied: servicex>=2.6.1 in /usr/local/lib/python3.10/site-packages (from func_adl_servicex) (2.8.0)\n",
      "Requirement already satisfied: make-it-sync in /usr/local/lib/python3.10/site-packages (from func-adl>=3.2->func_adl_servicex) (2.0.0)\n",
      "Requirement already satisfied: lark in /usr/local/lib/python3.10/site-packages (from qastle>=0.10->func_adl_servicex) (1.1.9)\n",
      "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (2.10)\n",
      "Requirement already satisfied: pandas>1.5 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (2.2.1)\n",
      "Requirement already satisfied: uproot>=4.0.1 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (5.3.2)\n",
      "Requirement already satisfied: backoff>=2.0 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (2.2.1)\n",
      "Requirement already satisfied: aiohttp~=3.6 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (3.9.3)\n",
      "Requirement already satisfied: minio~=5.0 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (5.0.10)\n",
      "Requirement already satisfied: tqdm~=4.0 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (4.66.1)\n",
      "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (2.32.0)\n",
      "Requirement already satisfied: confuse in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=1.0 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (15.0.0)\n",
      "Requirement already satisfied: awkward>=1.0.1 in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (2.6.3)\n",
      "Requirement already satisfied: dask-awkward in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (2024.3.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from servicex>=2.6.1->func_adl_servicex) (2024.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp~=3.6->servicex>=2.6.1->func_adl_servicex) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp~=3.6->servicex>=2.6.1->func_adl_servicex) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp~=3.6->servicex>=2.6.1->func_adl_servicex) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp~=3.6->servicex>=2.6.1->func_adl_servicex) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp~=3.6->servicex>=2.6.1->func_adl_servicex) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp~=3.6->servicex>=2.6.1->func_adl_servicex) (4.0.3)\n",
      "Requirement already satisfied: awkward-cpp==32 in /usr/local/lib/python3.10/site-packages (from awkward>=1.0.1->servicex>=2.6.1->func_adl_servicex) (32)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/site-packages (from awkward>=1.0.1->servicex>=2.6.1->func_adl_servicex) (7.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/site-packages (from awkward>=1.0.1->servicex>=2.6.1->func_adl_servicex) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from awkward>=1.0.1->servicex>=2.6.1->func_adl_servicex) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/site-packages (from awkward>=1.0.1->servicex>=2.6.1->func_adl_servicex) (4.10.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/site-packages (from minio~=5.0->servicex>=2.6.1->func_adl_servicex) (2.2.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.10/site-packages (from minio~=5.0->servicex>=2.6.1->func_adl_servicex) (2024.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from minio~=5.0->servicex>=2.6.1->func_adl_servicex) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/site-packages (from minio~=5.0->servicex>=2.6.1->func_adl_servicex) (2.9.0)\n",
      "Requirement already satisfied: configparser in /usr/local/lib/python3.10/site-packages (from minio~=5.0->servicex>=2.6.1->func_adl_servicex) (7.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas>1.5->servicex>=2.6.1->func_adl_servicex) (2024.1)\n",
      "Requirement already satisfied: cramjam>=2.5.0 in /usr/local/lib/python3.10/site-packages (from uproot>=4.0.1->servicex>=2.6.1->func_adl_servicex) (2.8.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from confuse->servicex>=2.6.1->func_adl_servicex) (6.0.1)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/site-packages (from dask-awkward->servicex>=2.6.1->func_adl_servicex) (5.3.3)\n",
      "Requirement already satisfied: dask>=2023.04.0 in /usr/local/lib/python3.10/site-packages (from dask-awkward->servicex>=2.6.1->func_adl_servicex) (2024.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth->servicex>=2.6.1->func_adl_servicex) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth->servicex>=2.6.1->func_adl_servicex) (4.9)\n",
      "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/site-packages (from dask>=2023.04.0->dask-awkward->servicex>=2.6.1->func_adl_servicex) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/site-packages (from dask>=2023.04.0->dask-awkward->servicex>=2.6.1->func_adl_servicex) (3.0.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/site-packages (from dask>=2023.04.0->dask-awkward->servicex>=2.6.1->func_adl_servicex) (1.4.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/site-packages (from dask>=2023.04.0->dask-awkward->servicex>=2.6.1->func_adl_servicex) (0.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->awkward>=1.0.1->servicex>=2.6.1->func_adl_servicex) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth->servicex>=2.6.1->func_adl_servicex) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil->minio~=5.0->servicex>=2.6.1->func_adl_servicex) (1.16.0)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/site-packages (from partd>=1.2.0->dask>=2023.04.0->dask-awkward->servicex>=2.6.1->func_adl_servicex) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install func_adl_servicex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a230d7b-5724-4a4f-8a8d-0f992a96fe07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: func_adl in /usr/local/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: make-it-sync in /usr/local/lib/python3.10/site-packages (from func_adl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install func_adl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b3fd2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'func_adl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mawkward\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mak\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcabinetry\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunc_adl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ObjectStream\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunc_adl_servicex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceXSourceUpROOT\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhist\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'func_adl'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "from func_adl import ObjectStream\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "import hist\n",
    "import mplhep\n",
    "import numpy as np\n",
    "import pyhf\n",
    "import uproot\n",
    "from servicex import ServiceXDataset\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.schemas.base import BaseSchema\n",
    "from coffea.dataset_tools import (\n",
    "    apply_to_fileset,\n",
    "    max_chunks,\n",
    "    preprocess,\n",
    ")\n",
    "\n",
    "import utils\n",
    "from utils import infofile  # contains cross-section information\n",
    "\n",
    "from hist.dask import Hist\n",
    "\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "\n",
    "utils.clean_up()  # delete output from previous runs of notebook (optional)\n",
    "utils.set_logging()  # configure logging output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b53ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set some global settings\n",
    "\n",
    "# chunk size to use\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# scaling for local setups with FuturesExecutor\n",
    "NUM_CORES = 4\n",
    "\n",
    "# ServiceX behavior: ignore cache with repeated queries\n",
    "#IGNORE_CACHE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551acd1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We are going to use the ATLAS Open Data for this demonstration, in particular a $H\\rightarrow ZZ^\\star$ analysis. Find more information on the [ATLAS Open Data documentation](http://opendata.atlas.cern/release/2020/documentation/physics/FL2.html) and in [ATL-OREACH-PUB-2020-001](https://cds.cern.ch/record/2707171). The datasets used are [10.7483/OPENDATA.ATLAS.2Y1T.TLGL](http://doi.org/10.7483/OPENDATA.ATLAS.2Y1T.TLGL). The material in this notebook is based on the [ATLAS Open Data notebooks](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata), a [PyHEP 2021 ServiceX demo](https://github.com/gordonwatts/pyhep-2021-SX-OpenDataDemo), and [Storm Lin's adoption](https://github.com/stormsomething/CoffeaHZZAnalysis) of this analysis.\n",
    "\n",
    "This notebook is meant as a **technical demonstration**. In particular, the systematic uncertainties defined are purely to demonstrate technical aspects of realistic workflows, and are not meant to be meaningful physically. The fit performed to data consequently also only demonstrate technical aspects. If you are interested about the physics of $H\\rightarrow ZZ^\\star$, check out for example the actual ATLAS cross-section measurement: [Eur. Phys. J. C 80 (2020) 942](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/HIGG-2018-29/).\n",
    "\n",
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](utils/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790c63f",
   "metadata": {},
   "source": [
    "### Tools and packages used in this example\n",
    "\n",
    "The notebook showcases:\n",
    "- data delivery with `ServiceX`\n",
    "- event / column selection with `func_adl`\n",
    "- data handling with `awkward-array`\n",
    "- histogram production with `coffea`\n",
    "- histogram handling with `hist`\n",
    "- visualization with `mplhep`, `hist` & `matplotlib`\n",
    "- ROOT file handling with `uproot`\n",
    "- statistical model construction with `cabinetry`\n",
    "- statistical inference with `pyhf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f9d84",
   "metadata": {},
   "source": [
    "### High-level strategy\n",
    "\n",
    "We will define which files to process, set up a query with `func_adl` to extract data provided by `ServiceX`, and use `coffea` to construct histograms.\n",
    "Those histograms will be saved with `uproot`, and then assembled into a statistical model with `cabinetry`.\n",
    "Following that, we perform statistical inference with `pyhf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c9ef0",
   "metadata": {},
   "source": [
    "### Required setup for this notebook\n",
    "\n",
    "If you are running on the coffea-casa Open Data instance, ServiceX credentials are automatically available to you.\n",
    "Otherwise you will need to set those up.\n",
    "Create a file `servicex.yaml` in your home directory, or the place this notebook is located in.\n",
    "\n",
    "See this [talk by KyungEon](https://indico.cern.ch/event/1076231/contributions/4560404/) and the [ServiceX doc](https://servicex.readthedocs.io/en/latest/user/getting-started/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6c272",
   "metadata": {},
   "source": [
    "## Files to process\n",
    "\n",
    "To get started, we define which files are going to be processed in this notebook.\n",
    "We also set some information for histogramming that will be used subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24197a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = (\n",
    "    \"http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/\"\n",
    ")\n",
    "\n",
    "# labels for combinations of datasets\n",
    "z_ttbar = r\"Background $Z,t\\bar{t}$\"\n",
    "zzstar = r\"Background $ZZ^{\\star}$\"\n",
    "signal = r\"Signal ($m_H$ = 125 GeV)\"\n",
    "\n",
    "input_files = {\n",
    "    \"Data\": [\n",
    "        prefix + \"data_A.4lep.root\",\n",
    "        prefix + \"data_B.4lep.root\",\n",
    "        prefix + \"data_C.4lep.root\",\n",
    "        prefix + \"data_D.4lep.root\",\n",
    "    ],\n",
    "    z_ttbar: [\n",
    "        prefix + \"mc_361106.Zee.4lep.root\",\n",
    "        prefix + \"mc_361107.Zmumu.4lep.root\",\n",
    "        prefix + \"mc_410000.ttbar_lep.4lep.root\",\n",
    "    ],\n",
    "    zzstar: [prefix + \"mc_363490.llll.4lep.root\"],\n",
    "    signal: [\n",
    "        prefix + \"mc_345060.ggH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"mc_344235.VBFH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"mc_341964.WH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"mc_341947.ZH125_ZZ4lep.4lep.root\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# information for histograms\n",
    "bin_edge_low = 80  # 80 GeV\n",
    "bin_edge_high = 250  # 250 GeV\n",
    "num_bins = 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bcc6e3",
   "metadata": {},
   "source": [
    "## Setting up a query with `func_adl`\n",
    "\n",
    "We are using `func_adl` for event & column selection, and make a datasource with the query built by `get_lepton_query`.\n",
    "\n",
    "A list of all available columns in the input files can be found in the [ATLAS documentation of branches](http://opendata.atlas.cern/release/2020/documentation/datasets/dataset13.html).\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> scale factor variation, applied already at event selection stage. Imagine that this could be a calculation that requires a lot of different variables which are no longer needed downstream afterwards, so it makes sense to do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31361726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lepton_query(source: ObjectStream) -> ObjectStream:\n",
    "    \"\"\"Performs event selection: require events with exactly four leptons.\n",
    "    Also select all columns needed further downstream for processing &\n",
    "    histogram filling.\n",
    "    \"\"\"\n",
    "    return source.Where(lambda event: event.lep_n == 4).Select(\n",
    "        lambda e: {\n",
    "            \"lep_pt\": e.lep_pt,\n",
    "            \"lep_eta\": e.lep_eta,\n",
    "            \"lep_phi\": e.lep_phi,\n",
    "            \"lep_energy\": e.lep_E,\n",
    "            \"lep_charge\": e.lep_charge,\n",
    "            \"lep_typeid\": e.lep_type,\n",
    "            \"mcWeight\": e.mcWeight,\n",
    "            \"scaleFactor\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP,\n",
    "            # scale factor systematic variation example\n",
    "            \"scaleFactorUP\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP\n",
    "            * 1.1,\n",
    "            \"scaleFactorDOWN\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP\n",
    "            * 0.9,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ddb0f0",
   "metadata": {},
   "source": [
    "# Caching the queried datasets with `ServiceX`\n",
    "\n",
    "Using the queries created with `func_adl`, we are using `ServiceX` to read the ATLAS Open Data files to build cached files with only the specific event information as dictated by the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682d3663",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# dummy dataset on which to generate the query\\ndummy_ds = ServiceXSourceUpROOT(\"cernopendata://dummy\", \"mini\", backend_name=\"uproot\")\\n\\n# tell low-level infrastructure not to contact ServiceX yet, only to\\n# return the qastle string it would have sent\\ndummy_ds.return_qastle = True\\n\\n# create the query\\nlepton_query = get_lepton_query(dummy_ds)\\nquery = lepton_query.value()\\n\\n# now we query the files and create a fileset dictionary containing the\\n# URLs pointing to the queried files\\n\\nt0 = time.time()\\n\\nfileset = {}\\n\\nfor ds_name in input_files.keys():\\n    ds = ServiceXDataset(input_files[ds_name], backend_name=\"uproot\", ignore_cache=IGNORE_CACHE)\\n    files = ds.get_data_rootfiles_uri(query, as_signed_url=True, title=ds_name)\\n\\n    fileset[ds_name] = {\"files\": [f.url for f in files],\\n                        \"metadata\": {\"dataset_name\": ds_name}\\n                       }\\n\\nprint(f\"execution took {time.time() - t0:.2f} seconds\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# dummy dataset on which to generate the query\n",
    "dummy_ds = ServiceXSourceUpROOT(\"cernopendata://dummy\", \"mini\", backend_name=\"uproot\")\n",
    "\n",
    "# tell low-level infrastructure not to contact ServiceX yet, only to\n",
    "# return the qastle string it would have sent\n",
    "dummy_ds.return_qastle = True\n",
    "\n",
    "# create the query\n",
    "lepton_query = get_lepton_query(dummy_ds)\n",
    "query = lepton_query.value()\n",
    "\n",
    "# now we query the files and create a fileset dictionary containing the\n",
    "# URLs pointing to the queried files\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "fileset = {}\n",
    "\n",
    "for ds_name in input_files.keys():\n",
    "    ds = ServiceXDataset(input_files[ds_name], backend_name=\"uproot\", ignore_cache=IGNORE_CACHE)\n",
    "    files = ds.get_data_rootfiles_uri(query, as_signed_url=True, title=ds_name)\n",
    "\n",
    "    fileset[ds_name] = {\"files\": [f.url for f in files],\n",
    "                        \"metadata\": {\"dataset_name\": ds_name}\n",
    "                       }\n",
    "\n",
    "print(f\"execution took {time.time() - t0:.2f} seconds\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d9c5f",
   "metadata": {},
   "source": [
    "We now have a fileset dictionary containing the addresses of the queried files, ready to pass to `coffea`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf95576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fileset'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''fileset'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53de594",
   "metadata": {},
   "source": [
    "## Processing `ServiceX`-provided data with `coffea`\n",
    "\n",
    "Event weighting: look up cross-section from a provided utility file, and correctly normalize all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be2a6ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xsec_weight(sample: str) -> float:\n",
    "    \"\"\"Returns normalization weight for a given sample.\"\"\"\n",
    "    lumi = 10_000  # pb^-1\n",
    "    xsec_map = infofile.infos[sample]  # dictionary with event weighting information\n",
    "    xsec_weight = (lumi * xsec_map[\"xsec\"]) / (xsec_map[\"sumw\"] * xsec_map[\"red_eff\"])\n",
    "    return xsec_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b14ccb",
   "metadata": {},
   "source": [
    "Cuts to apply:\n",
    "- two opposite flavor lepton pairs (total lepton charge is 0)\n",
    "- lepton types: 4 electrons, 4 muons, or 2 electrons + 2 muons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1515ec6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lepton_filter(lep_charge, lep_type):\n",
    "    \"\"\"Filters leptons: sum of charges is required to be 0, and sum of lepton types 44/48/52.\n",
    "    Electrons have type 11, muons have 13, so this means 4e/4mu/2e2mu.\n",
    "    \"\"\"\n",
    "    sum_lep_charge = ak.sum(lep_charge, axis=1)\n",
    "    sum_lep_type = ak.sum(lep_type, axis=1)\n",
    "    good_lep_type = ak.any(\n",
    "        [sum_lep_type == 44, sum_lep_type == 48, sum_lep_type == 52], axis=0\n",
    "    )\n",
    "    return ak.all([sum_lep_charge == 0, good_lep_type], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfd064",
   "metadata": {},
   "source": [
    "Set up the `coffea` processor. It will apply cuts, calculate the four-lepton invariant mass, and fill a histogram.\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> m4l variation, applied in the processor to remaining events. This might instead for example be the result of applying a tool performing a computationally expensive calculation, which should only be run for events where it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57124838-703c-44d3-af54-25f149b06ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HZZProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self, events):\n",
    "        dataset_category = events.metadata['dataset']\n",
    "        isRealData = \"genWeight\" not in events.fields\n",
    "        sumw = 0. if isRealData else ak.sum(events.genWeight, axis=0)\n",
    "        cutflow = {\"start\": ak.num(events, axis=0)}\n",
    "        \n",
    "#        if isRealData:\n",
    "#            events = events[\n",
    "#                corrections.lumimask(events.run, events.luminosityBlock)\n",
    "#            ]\n",
    "#            cutflow[\"lumimask\"] = ak.num(events, axis=0)\n",
    "    \n",
    "        # apply a cut to events, based on lepton charge and lepton type\n",
    "        events[\"lepfiltered\"] = events[lepton_filter(events.lep_charge, events.lep_type)]\n",
    "\n",
    "        # construct lepton four-vectors\n",
    "        leptons = ak.zip(\n",
    "            {\"pt\": events.lep_pt,\n",
    "             \"eta\": events.lep_eta,\n",
    "             \"phi\": events.lep_phi,\n",
    "             \"energy\": events.lep_energy},\n",
    "            with_name=\"Momentum4D\",\n",
    "        )\n",
    "\n",
    "        cutflow[\"ossf\"] = ak.num(events, axis=0)\n",
    "        \n",
    "        mllll = (leptons[:, 0] + leptons[:, 1] + leptons[:, 2] + leptons[:, 3]\n",
    "        ).mass / 1000\n",
    "        \n",
    "        # creat histogram holding outputs, for data just binned in m4l\n",
    "        mllllhist_data = hist.Hist.new.Reg(\n",
    "            num_bins,\n",
    "            bin_edge_low,\n",
    "            bin_edge_high,\n",
    "            name=\"mllll\",\n",
    "            label=\"$\\mathrm{m_{4l}}$ [GeV]\",\n",
    "        ).Weight()  # using weighted storage here for plotting later, but not needed\n",
    "\n",
    "        # three histogram axes for MC: m4l, category, and variation (nominal and\n",
    "        # systematic variations)\n",
    "        mllllhist_MC = (\n",
    "            hist.Hist.new.Reg(\n",
    "                num_bins,\n",
    "                bin_edge_low,\n",
    "                bin_edge_high,\n",
    "                name=\"mllll\",\n",
    "                label=\"$\\mathrm{m_{4l}}$ [GeV]\",\n",
    "            )\n",
    "            .StrCat([k for k in fileset.keys() if k != \"Data\"], name=\"dataset\")\n",
    "            .StrCat(\n",
    "                [\"nominal\", \"scaleFactorUP\", \"scaleFactorDOWN\", \"m4lUP\", \"m4lDOWN\"],\n",
    "                name=\"variation\",\n",
    "            )\n",
    "            .Weight()\n",
    "        )\n",
    "        \n",
    "        if dataset_category == \"Data\":\n",
    "            # create and fill a histogram for m4l\n",
    "            mllllhist_data.fill(mllll=mllll)\n",
    "\n",
    "        else:\n",
    "                # extract the sample name from the filename to calculate x-sec weight\n",
    "                sample = re.findall(r\"mc_\\d+\\.(.+)\\.4lep\", events.metadata[\"filename\"])[0]\n",
    "                basic_weight = get_xsec_weight(sample) * events.mcWeight\n",
    "                totalWeights = basic_weight * events.scaleFactor\n",
    "\n",
    "                # calculate systematic variations for weight\n",
    "                totalWeightsUp = basic_weight * events.scaleFactorUP\n",
    "                totalWeightsDown = basic_weight * events.scaleFactorDOWN\n",
    "\n",
    "                # create and fill weighted histograms for m4l: nominal and variations\n",
    "                mllllhist_MC.fill(\n",
    "                    mllll=mllll,\n",
    "                    dataset=dataset_category,\n",
    "                    variation=\"nominal\",\n",
    "                    weight=totalWeights,\n",
    "                )\n",
    "\n",
    "                # scale factor variations\n",
    "                mllllhist_MC.fill(\n",
    "                    mllll=mllll,\n",
    "                    dataset=dataset_category,\n",
    "                    variation=\"scaleFactorUP\",\n",
    "                    weight=totalWeightsUp,\n",
    "                )\n",
    "                mllllhist_MC.fill(\n",
    "                    mllll=mllll,\n",
    "                    dataset=dataset_category,\n",
    "                    variation=\"scaleFactorDOWN\",\n",
    "                    weight=totalWeightsDown,\n",
    "                )\n",
    "\n",
    "                # variation in 4-lepton invariant mass\n",
    "                mllllhist_MC.fill(\n",
    "                    mllll=mllll * 1.01,\n",
    "                    dataset=dataset_category,\n",
    "                    variation=\"m4lUP\",\n",
    "                    weight=totalWeights,\n",
    "                )\n",
    "                mllllhist_MC.fill(\n",
    "                    mllll=mllll * 0.99,\n",
    "                    dataset=dataset_category,\n",
    "                    variation=\"m4lDOWN\",\n",
    "                    weight=totalWeights,\n",
    "                )\n",
    "\n",
    "        return {\"data\": mllllhist_data, \"MC\": mllllhist_MC}\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bbc8b6b-9948-45e2-b10d-804fe21482d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data': ['http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/data_A.4lep.root',\n",
       "  'http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/data_B.4lep.root',\n",
       "  'http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/data_C.4lep.root',\n",
       "  'http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/data_D.4lep.root'],\n",
       " 'Background $Z,t\\\\bar{t}$': ['http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/mc_361106.Zee.4lep.root',\n",
       "  'http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/mc_361107.Zmumu.4lep.root',\n",
       "  'http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/mc_410000.ttbar_lep.4lep.root'],\n",
       " 'Background $ZZ^{\\\\star}$': ['http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/mc_363490.llll.4lep.root'],\n",
       " 'Signal ($m_H$ = 125 GeV)': ['http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/mc_345060.ggH125_ZZ4lep.4lep.root',\n",
       "  'http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/mc_344235.VBFH125_ZZ4lep.4lep.root',\n",
       "  'http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/mc_341964.WH125_ZZ4lep.4lep.root',\n",
       "  'http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/mc_341947.ZH125_ZZ4lep.4lep.root']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2fd0544-539c-4954-a13f-7788840d88bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"atlas_inputs.json\", \"rt\") as file:\n",
    "    initial_fileset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a45dad-f08b-4153-9d3e-e798e07deaba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_available, preprocessed_total = preprocess(\n",
    "        initial_fileset,\n",
    "        step_size=100_000,\n",
    "        align_clusters=None,\n",
    "        skip_bad_files=True,\n",
    "        recalculate_steps=False,\n",
    "        files_per_batch=1,\n",
    "        file_exceptions=(OSError,),\n",
    "        save_form=True,\n",
    "        uproot_options={},\n",
    "        step_size_safety_factor=0.5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be2a221-7dac-43d0-b82b-bc5e0e57d147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert typetracer arrays to Python lists\n\nThis error occurred while calling\n\n    ak.any(\n        [dask.awkward<equal, npartitions=4>, dask.awkward<equal, npartitions=...\n        axis = 0\n    )",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/_dispatch.py:62\u001b[0m, in \u001b[0;36mnamed_high_level_function.<locals>.dispatch\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen_or_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/operations/ak_any.py:66\u001b[0m, in \u001b[0;36many\u001b[0;34m(array, axis, keepdims, mask_identity, highlevel, behavior, attrs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Implementation\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_identity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehavior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/operations/ak_any.py:72\u001b[0m, in \u001b[0;36m_impl\u001b[0;34m(array, axis, keepdims, mask_identity, highlevel, behavior, attrs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HighLevelContext(behavior\u001b[38;5;241m=\u001b[39mbehavior, attrs\u001b[38;5;241m=\u001b[39mattrs) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[0;32m---> 72\u001b[0m     layout \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimitive_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m reducer \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39m_reducers\u001b[38;5;241m.\u001b[39mAny()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/_layout.py:145\u001b[0m, in \u001b[0;36mHighLevelContext.unwrap\u001b[0;34m(self, obj, allow_record, allow_unknown, none_policy, primitive_policy, string_policy, use_from_iter, regulararray)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(obj)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_layout_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnone_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnone_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_from_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_from_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimitive_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimitive_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstring_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregulararray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregulararray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/operations/ak_to_layout.py:263\u001b[0m, in \u001b[0;36m_impl\u001b[0;34m(obj, allow_record, allow_unknown, none_policy, regulararray, use_from_iter, primitive_policy, string_policy)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_from_iter:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mak\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_record\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/_dispatch.py:39\u001b[0m, in \u001b[0;36mnamed_high_level_function.<locals>.dispatch\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OperationErrorContext(name, args, kwargs):\n\u001b[0;32m---> 39\u001b[0m     gen_or_result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isgenerator(gen_or_result):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/operations/ak_from_iter.py:70\u001b[0m, in \u001b[0;36mfrom_iter\u001b[0;34m(iterable, allow_record, highlevel, behavior, attrs, initial, resize)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    iterable (Python iterable): Data to convert into an Awkward Array.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mSee also #ak.to_list.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehavior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_record\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/operations/ak_from_iter.py:100\u001b[0m, in \u001b[0;36m_impl\u001b[0;34m(iterable, highlevel, behavior, allow_record, initial, resize, attrs)\u001b[0m\n\u001b[1;32m     99\u001b[0m builder \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mArrayBuilder(initial\u001b[38;5;241m=\u001b[39minitial, resize\u001b[38;5;241m=\u001b[39mresize)\n\u001b[0;32m--> 100\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromiter\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m formstr, length, buffers \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mto_buffers()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask_awkward/lib/core.py:1527\u001b[0m, in \u001b[0;36mArray.__getattr__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(cls_method)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_BehaviorMethodFn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyphenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask_awkward/lib/core.py:1588\u001b[0m, in \u001b[0;36mArray.map_partitions\u001b[0;34m(self, func, traverse, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map a function across all partitions of the collection.\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m \n\u001b[1;32m   1564\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \n\u001b[1;32m   1587\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask_awkward/lib/core.py:2107\u001b[0m, in \u001b[0;36mmap_partitions\u001b[0;34m(base_fn, label, token, meta, output_divisions, traverse, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m fn \u001b[38;5;241m=\u001b[39m ArgsKwargsPackedFunction(\n\u001b[1;32m   2102\u001b[0m     base_fn,\n\u001b[1;32m   2103\u001b[0m     arg_repackers,\n\u001b[1;32m   2104\u001b[0m     kwarg_repacker,\n\u001b[1;32m   2105\u001b[0m     arg_lens_for_repackers,\n\u001b[1;32m   2106\u001b[0m )\n\u001b[0;32m-> 2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_partitions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg_flat_deps_expanded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwarg_flat_deps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2114\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_divisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask_awkward/lib/core.py:1943\u001b[0m, in \u001b[0;36m_map_partitions\u001b[0;34m(fn, label, token, meta, output_divisions, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1943\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mmap_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1945\u001b[0m hlg \u001b[38;5;241m=\u001b[39m HighLevelGraph\u001b[38;5;241m.\u001b[39mfrom_collections(\n\u001b[1;32m   1946\u001b[0m     name,\n\u001b[1;32m   1947\u001b[0m     lay,\n\u001b[1;32m   1948\u001b[0m     dependencies\u001b[38;5;241m=\u001b[39mdeps,\n\u001b[1;32m   1949\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask_awkward/lib/core.py:2467\u001b[0m, in \u001b[0;36mmap_meta\u001b[0;34m(fn, *deps)\u001b[0m\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2467\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meta\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask_awkward/lib/core.py:1903\u001b[0m, in \u001b[0;36mArgsKwargsPackedFunction.__call__\u001b[0;34m(self, *args_deps_expanded)\u001b[0m\n\u001b[1;32m   1902\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwarg_repacker(args_deps_expanded[len_args:])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask_awkward/lib/core.py:2567\u001b[0m, in \u001b[0;36m_BehaviorMethodFn.__call__\u001b[0;34m(self, coll, *args)\u001b[0m\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, coll: ak\u001b[38;5;241m.\u001b[39mArray, \u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ak\u001b[38;5;241m.\u001b[39mArray:\n\u001b[0;32m-> 2567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/highlevel.py:499\u001b[0m, in \u001b[0;36mArray.to_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;124;03mConverts this Array into Python objects; same as #ak.to_list.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_behavior\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/contents/content.py:1082\u001b[0m, in \u001b[0;36mContent.to_list\u001b[0;34m(self, behavior)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, behavior: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbehavior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/contents/numpyarray.py:1309\u001b[0m, in \u001b[0;36mNumpyArray._to_list\u001b[0;34m(self, behavior, json_conversions)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mnplike\u001b[38;5;241m.\u001b[39mknown_data:\n\u001b[0;32m-> 1309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert typetracer arrays to Python lists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbyte\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert typetracer arrays to Python lists",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m small_tg, small_rep \u001b[38;5;241m=\u001b[39m \u001b[43mapply_to_fileset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_manipulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHZZProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfileset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_fileset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mschemaclass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBaseSchema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muproot_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow_read_errors_with_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;167;43;01mOSError\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;167;43;01mKeyError\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/coffea/dataset_tools/apply_processor.py:125\u001b[0m, in \u001b[0;36mapply_to_fileset\u001b[0;34m(data_manipulation, fileset, schemaclass, uproot_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    124\u001b[0m metadata\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[0;32m--> 125\u001b[0m dataset_out \u001b[38;5;241m=\u001b[39m \u001b[43mapply_to_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_manipulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschemaclass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muproot_options\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_out) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    129\u001b[0m     out[name], report[name] \u001b[38;5;241m=\u001b[39m dataset_out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/coffea/dataset_tools/apply_processor.py:81\u001b[0m, in \u001b[0;36mapply_to_dataset\u001b[0;34m(data_manipulation, dataset, schemaclass, metadata, uproot_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_manipulation, ProcessorABC):\n\u001b[0;32m---> 81\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdata_manipulation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_manipulation, Callable):\n\u001b[1;32m     83\u001b[0m     out \u001b[38;5;241m=\u001b[39m data_manipulation(events)\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mHZZProcessor.process\u001b[0;34m(self, events)\u001b[0m\n\u001b[1;32m      9\u001b[0m         cutflow \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: ak\u001b[38;5;241m.\u001b[39mnum(events, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#        if isRealData:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#            events = events[\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#                corrections.lumimask(events.run, events.luminosityBlock)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# apply a cut to events, based on lepton charge and lepton type\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m         events[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlepfiltered\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m events[\u001b[43mlepton_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlep_charge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlep_type\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# construct lepton four-vectors\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         leptons \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mzip(\n\u001b[1;32m     22\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: events\u001b[38;5;241m.\u001b[39mlep_pt,\n\u001b[1;32m     23\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m\"\u001b[39m: events\u001b[38;5;241m.\u001b[39mlep_eta,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m             with_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMomentum4D\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mlepton_filter\u001b[0;34m(lep_charge, lep_type)\u001b[0m\n\u001b[1;32m      5\u001b[0m sum_lep_charge \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39msum(lep_charge, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m sum_lep_type \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39msum(lep_type, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m good_lep_type \u001b[38;5;241m=\u001b[39m \u001b[43mak\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msum_lep_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_lep_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_lep_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m52\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ak\u001b[38;5;241m.\u001b[39mall([sum_lep_charge \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, good_lep_type], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/_dispatch.py:38\u001b[0m, in \u001b[0;36mnamed_high_level_function.<locals>.dispatch\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# NOTE: this decorator assumes that the operation is exposed under `ak.`\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m OperationErrorContext(name, args, kwargs):\n\u001b[1;32m     39\u001b[0m         gen_or_result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m isgenerator(gen_or_result):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/_errors.py:85\u001b[0m, in \u001b[0;36mErrorContext.__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Handle caught exception\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         exception_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(exception_type, \u001b[38;5;167;01mException\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     84\u001b[0m     ):\n\u001b[0;32m---> 85\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexception_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Step out of the way so that another ErrorContext can become primary.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/_errors.py:95\u001b[0m, in \u001b[0;36mErrorContext.handle_exception\u001b[0;34m(self, cls, exception)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecorate_exception(\u001b[38;5;28mcls\u001b[39m, exception)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecorate_exception(\u001b[38;5;28mcls\u001b[39m, exception)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert typetracer arrays to Python lists\n\nThis error occurred while calling\n\n    ak.any(\n        [dask.awkward<equal, npartitions=4>, dask.awkward<equal, npartitions=...\n        axis = 0\n    )"
     ]
    }
   ],
   "source": [
    "small_tg, small_rep = apply_to_fileset(data_manipulation=HZZProcessor(),\n",
    "                            fileset=initial_fileset,\n",
    "                            schemaclass=BaseSchema,\n",
    "                            uproot_options={\"allow_read_errors_with_report\": (OSError, KeyError)},\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48872a1-9773-4209-954c-2754a3ca6eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_compute = apply_to_fileset(\n",
    "                HZZProcessor(),\n",
    "                max_chunks(initial_fileset, 300),\n",
    "                schemaclass=BaseSchema,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2ee2-3506-4c1b-90a8-1f8d41865e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(out,) = dask.compute(to_compute)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c2ad5",
   "metadata": {},
   "source": [
    "## Producing the desired histograms\n",
    "\n",
    "Run the processor on data previously gathered by ServiceX, then gather output histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9ce14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "executor = processor.FuturesExecutor(workers=NUM_CORES)\n",
    "run = processor.Runner(executor=executor, savemetrics=True, metadata_cache={},\n",
    "                       chunksize=CHUNKSIZE, schema=BaseSchema)\n",
    "all_histograms, metrics = run(fileset, \"servicex\", processor_instance=HZZAnalysis())\n",
    "\n",
    "print(f\"execution took {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62b89d",
   "metadata": {},
   "source": [
    "## Plotting histograms with `mplhep`, `hist` & `matplotlib`\n",
    "\n",
    "We can plot some of the histograms we just produced with `mplhep`, `hist` & `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms with mplhep & hist\n",
    "mplhep.histplot(\n",
    "    all_histograms[\"data\"], histtype=\"errorbar\", color=\"black\", label=\"Data\"\n",
    ")\n",
    "hist.Hist.plot1d(\n",
    "    all_histograms[\"MC\"][:, :, \"nominal\"],\n",
    "    stack=True,\n",
    "    histtype=\"fill\",\n",
    "    color=[\"purple\", \"red\", \"lightblue\"],\n",
    ")\n",
    "\n",
    "# plot band for MC statistical uncertainties via utility function\n",
    "# (this uses matplotlib directly)\n",
    "utils.plot_errorband(bin_edge_low, bin_edge_high, num_bins, all_histograms)\n",
    "\n",
    "# we are using a small utility function to also save the figure in .png and .pdf\n",
    "# format, you can find the produced figure in the figures/ folder\n",
    "utils.save_figure(\"m4l_stat_uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3d5b5",
   "metadata": {},
   "source": [
    "## Saving histograms with `uproot`\n",
    "\n",
    "In order to build a statistical model, we will use `cabinetry`'s support for reading histograms to build a so-called workspace specifying the model.\n",
    "We will save the histograms we just created to disk with `uproot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75837f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"histograms.root\"\n",
    "with uproot.recreate(file_name) as f:\n",
    "    f[\"data\"] = all_histograms[\"data\"]\n",
    "\n",
    "    f[\"Z_tt\"] = all_histograms[\"MC\"][:, z_ttbar, \"nominal\"]\n",
    "    f[\"Z_tt_SF_up\"] = all_histograms[\"MC\"][:, z_ttbar, \"scaleFactorUP\"]\n",
    "    f[\"Z_tt_SF_down\"] = all_histograms[\"MC\"][:, z_ttbar, \"scaleFactorDOWN\"]\n",
    "    f[\"Z_tt_m4l_up\"] = all_histograms[\"MC\"][:, z_ttbar, \"m4lUP\"]\n",
    "    f[\"Z_tt_m4l_down\"] = all_histograms[\"MC\"][:, z_ttbar, \"m4lDOWN\"]\n",
    "\n",
    "    f[\"ZZ\"] = all_histograms[\"MC\"][:, zzstar, \"nominal\"]\n",
    "    f[\"ZZ_SF_up\"] = all_histograms[\"MC\"][:, zzstar, \"scaleFactorUP\"]\n",
    "    f[\"ZZ_SF_down\"] = all_histograms[\"MC\"][:, zzstar, \"scaleFactorDOWN\"]\n",
    "    f[\"ZZ_m4l_up\"] = all_histograms[\"MC\"][:, zzstar, \"m4lUP\"]\n",
    "    f[\"ZZ_m4l_down\"] = all_histograms[\"MC\"][:, zzstar, \"m4lDOWN\"]\n",
    "\n",
    "    f[\"signal\"] = all_histograms[\"MC\"][:, signal, \"nominal\"]\n",
    "    f[\"signal_SF_up\"] = all_histograms[\"MC\"][:, signal, \"scaleFactorUP\"]\n",
    "    f[\"signal_SF_down\"] = all_histograms[\"MC\"][:, signal, \"scaleFactorDOWN\"]\n",
    "    f[\"signal_m4l_up\"] = all_histograms[\"MC\"][:, signal, \"m4lUP\"]\n",
    "    f[\"signal_m4l_down\"] = all_histograms[\"MC\"][:, signal, \"m4lDOWN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac08fe",
   "metadata": {},
   "source": [
    "## Building a workspace and running a fit with `cabinetry` & `pyhf`\n",
    "\n",
    "Take a look at the `cabinetry` configuration file in `config.yml`.\n",
    "It specifies the model to be built.\n",
    "In particular, it lists the samples we are going to be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cabinetry.configuration.load(\"config.yml\")\n",
    "config[\"Samples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb046e9a",
   "metadata": {},
   "source": [
    "It also shows which systematic uncertainties we will apply.\n",
    "This includes a new systematic uncertainty defined at this stage.\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> $ZZ^\\star$ normalization; this does not require any histograms, so we can define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"Systematics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d1c08",
   "metadata": {},
   "source": [
    "The information in the configuration is used to construct a statistical model, the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f203255",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.templates.collect(config)\n",
    "cabinetry.templates.postprocess(config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc73ee",
   "metadata": {},
   "source": [
    "Create a `pyhf` model and extract the data from the workspace. Perform a MLE fit, the results will be reported in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415afd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a391874",
   "metadata": {},
   "source": [
    "We can visualize the pulls and correlations.\n",
    "`cabinetry` saves this figure by default as a `.pdf`, but here we will use our small utility again to save in both `.png` and `.pdf` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"Signal_norm\", close_figure=False, save_figure=False\n",
    ")\n",
    "utils.save_figure(\"pulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28435ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.visualize.correlation_matrix(\n",
    "    fit_results, pruning_threshold=0.15, close_figure=False, save_figure=False\n",
    ")\n",
    "utils.save_figure(\"correlation_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0738453",
   "metadata": {},
   "source": [
    "Finally, visualize the post-fit model and data.\n",
    "We first create the post-fit model prediction, using the model and the best-fit resuts.\n",
    "\n",
    "The visualization is using information stored in the workspace, which does not include binning or which observable is used.\n",
    "This information can be passed in via the `config` kwarg, but we can also edit the figure after its creation.\n",
    "We will demonstrate both approaches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create post-fit model prediction\n",
    "postfit_model = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "\n",
    "# binning to use in plot\n",
    "plot_config = {\n",
    "    \"Regions\": [\n",
    "        {\n",
    "            \"Name\": \"Signal_region\",\n",
    "            \"Binning\": list(np.linspace(bin_edge_low, bin_edge_high, num_bins + 1)),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "figure_dict = cabinetry.visualize.data_mc(\n",
    "    postfit_model, data, config=plot_config, save_figure=False\n",
    ")\n",
    "\n",
    "# modify x-axis label\n",
    "fig = figure_dict[0][\"figure\"]\n",
    "fig.axes[1].set_xlabel(\"m4l [GeV]\")\n",
    "\n",
    "# let's also save the figure\n",
    "utils.save_figure(\"Signal_region_postfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b96170",
   "metadata": {},
   "source": [
    "We can also use `pyhf` directly. We already have a model and data, so let's calculate the CLs value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_test = 1.0\n",
    "cls = float(pyhf.infer.hypotest(mu_test, data, model))\n",
    "print(f\"CL_S for Signal_norm={mu_test} is {cls:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58aaca4",
   "metadata": {},
   "source": [
    "## Final remarks: control flow and duplication of information\n",
    "\n",
    "In the demonstration today we first built histograms, making sure we produce all that we will need for our statistical model.\n",
    "We then built the model, saying where to find each histogram.\n",
    "If we want to add a new histogram-based systematic uncertainty, we will need to implement it in two places:\n",
    "- in the histogram-producing code (e.g. coffea processor),\n",
    "- in the model building instructions.\n",
    "\n",
    "It can be convenient to avoid this duplication, which can be achieved as follows:\n",
    "- specify all relevant information in the model building configuration,\n",
    "- use that information to steer the histogram production.\n",
    "\n",
    "This method is available in `cabinetry`, check out the [cabinetry tutorials repository](https://github.com/cabinetry/cabinetry-tutorials) to see it in action.\n",
    "With this approach `cabinetry` builds the histograms it needs itself.\n",
    "\n",
    "This works well for some cases, but not for others:\n",
    "- a simple cut can be specified easily in the configuration, and varied for systematic uncertainties,\n",
    "- a complex calculation cannot easily be captured in such a way.\n",
    "\n",
    "The image below describes different options.\n",
    "The general considerations here are independent of the exact tools used, and should apply equally when using similar workflows.\n",
    "\n",
    "![schematic for control flow options](utils/control_flow.png)\n",
    "\n",
    "A flexible design that works well for all scenarios is needed here, but is not available yet.\n",
    "If you have thoughts about this, please do get in touch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166163d",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are a few ideas to try out to become more familiar with the tools shown in this notebook:\n",
    "\n",
    "- Run the notebook a second time. It should be faster now — you are taking advantage of caching!\n",
    "- Change the event pre-selection in the `func_adl` query. Try out requiring exactly zero jets. Has the analysis become more or less sensitive after this change?\n",
    "- Change the lepton requirements. What happens when only accepting events with 4 electrons or 4 muons?\n",
    "- Try a different binning. Note how you still benefit from caching in the `ServiceX`-delivered data!\n",
    "- Compare the pre- and post-fit data/MC agreement (hint: the `fit_results` kwarg in `cabinetry.model_utils.prediction` is optional).\n",
    "- Find the value of the `Signal_norm` normalization factor for which CL_S = 0.05, the 95% confidence level upper parameter limit (hint: use `pyhf` directly, or `cabinetry.fit.limit`).\n",
    "- Separate the $Z$ and $t\\bar{t}$ backgrounds and add a 6% normalization uncertainty for $t\\bar{t}$ in the fit.\n",
    "- Replace the 10% normalization uncertainty for the $ZZ^\\star$ background by a free-floating normalization factor in the fit.\n",
    "\n",
    "\n",
    "Advanced ideas:\n",
    "- Implement a more realistic systematic uncertainty in the `coffea` processor, for example for detector-related uncertainties for lepton kinematics. Propagate it through the analysis chain to observe the impact in a ranking plot produced with `cabinetry`.\n",
    "- Try out this workflow with your own analysis! Are there missing features or ways to streamline the experience? If so, please let us know!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
